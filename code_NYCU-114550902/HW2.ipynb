{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3ff814ca",
      "metadata": {
        "id": "3ff814ca"
      },
      "source": [
        "導入庫"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5UgBSSKD-V1M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UgBSSKD-V1M",
        "outputId": "70eae7f9-44e3-4634-dc79-6b6b98182a7a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9af913f",
      "metadata": {
        "id": "a9af913f"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "把 HW2 的 <class,x,y,w,h> 像素标签，转换成 YOLO 归一化标签，写到 data_yolo 结构，并生成 dataset.yaml\n",
        "- 输入：\n",
        "    data/train/*.png + 同名 .txt（像素坐标）\n",
        "    data/test/*.png\n",
        "- 输出：\n",
        "    data_yolo/\n",
        "      images/{train,val,test}/\n",
        "      labels/{train,val}/\n",
        "      dataset.yaml\n",
        "\"\"\"\n",
        "import random, shutil\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "# ==== 路径与类名 ====\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/Colab Notebooks/cv/hw2/taica-cvpdl-2025-hw-2 (Unzipped Files)/CVPDL_hw2/CVPDL_hw2\")\n",
        "TRAIN_IMG = DATA_ROOT / \"train\"\n",
        "TEST_IMG  = DATA_ROOT / \"test\"\n",
        "\n",
        "YOLO_ROOT = Path(\"data_yolo\")\n",
        "IMAGES_TRAIN = YOLO_ROOT / \"images\" / \"train\"\n",
        "IMAGES_VAL   = YOLO_ROOT / \"images\" / \"val\"\n",
        "IMAGES_TEST  = YOLO_ROOT / \"images\" / \"test\"\n",
        "LABELS_TRAIN = YOLO_ROOT / \"labels\" / \"train\"\n",
        "LABELS_VAL   = YOLO_ROOT / \"labels\" / \"val\"\n",
        "DATA_YAML    = YOLO_ROOT / \"dataset.yaml\"\n",
        "\n",
        "NAMES = [\"car\",\"hov\",\"person\",\"motorcycle\"]\n",
        "NAME2ID = {n:i for i,n in enumerate(NAMES)}\n",
        "VAL_RATIO = 0.1\n",
        "SEED = 42\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JoAtgyKnbtmy",
      "metadata": {
        "id": "JoAtgyKnbtmy"
      },
      "source": [
        "轉換成yolo可讀格式資料"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2lPJq59XbyLi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lPJq59XbyLi",
        "outputId": "657d980a-868c-4200-b2fe-afd0149bfdf8"
      },
      "outputs": [],
      "source": [
        "def ensure_dirs():\n",
        "    for p in [IMAGES_TRAIN, IMAGES_VAL, IMAGES_TEST, LABELS_TRAIN, LABELS_VAL]:\n",
        "        p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def parse_per_image_txt(txt_path: Path):\n",
        "    \"\"\"\n",
        "    读取每图同名 .txt：每行 <class> <x> <y> <w> <h> （像素）\n",
        "    class 可为名字或 0~3 整数\n",
        "    \"\"\"\n",
        "    items = []\n",
        "    if not txt_path.exists():\n",
        "        return items\n",
        "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for ln in f:\n",
        "            ln = ln.strip().replace(\",\", \" \").replace(\"\\t\",\" \")\n",
        "            if not ln:\n",
        "                continue\n",
        "            parts = [p for p in ln.split() if p]\n",
        "            if len(parts) < 5:  # 容错：空标注也允许\n",
        "                continue\n",
        "            cls_token = parts[0]\n",
        "            if cls_token.isdigit():\n",
        "                cid = int(cls_token)\n",
        "            else:\n",
        "                cid = NAME2ID.get(cls_token, None)\n",
        "                if cid is None:\n",
        "                    # 非 4 类的行直接忽略\n",
        "                    continue\n",
        "            x, y, w, h = map(float, parts[1:5])\n",
        "            items.append((cid, x, y, w, h))\n",
        "    return items\n",
        "\n",
        "def to_yolo_line(cid, bbox, W, H):\n",
        "    # 像素 (x,y,w,h)-> 归一化 (cx,cy,w,h) in [0,1], 左上→中心\n",
        "    x, y, w, h = bbox\n",
        "    cx = (x + w/2.0) / W\n",
        "    cy = (y + h/2.0) / H\n",
        "    return f\"{cid} {cx:.6f} {cy:.6f} {w/W:.6f} {h/H:.6f}\"\n",
        "\n",
        "def split_train_val(files, ratio=0.1, seed=42):\n",
        "    files = sorted(files)\n",
        "    rng = random.Random(seed)\n",
        "    n = len(files)\n",
        "    k = int(n * ratio)\n",
        "    val = set(rng.sample(files, k)) if k > 0 else set()\n",
        "    train = [f for f in files if f not in val]\n",
        "    val = list(val)\n",
        "    return train, val\n",
        "\n",
        "# ==== 执行 ====\n",
        "ensure_dirs()\n",
        "img_exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".webp\"}\n",
        "\n",
        "train_imgs = [p for p in TRAIN_IMG.iterdir() if p.suffix.lower() in img_exts]\n",
        "test_imgs  = [p for p in TEST_IMG.iterdir()  if p.suffix.lower() in img_exts]\n",
        "assert train_imgs, f\"找不到训练图片于 {TRAIN_IMG}\"\n",
        "\n",
        "train_split, val_split = split_train_val(train_imgs, VAL_RATIO, SEED)\n",
        "print(f\"Train: {len(train_split)}, Val: {len(val_split)}, Test: {len(test_imgs)}\")\n",
        "\n",
        "for split_files, img_out, lbl_out in [\n",
        "    (train_split, IMAGES_TRAIN, LABELS_TRAIN),\n",
        "    (val_split,   IMAGES_VAL,   LABELS_VAL),\n",
        "]:\n",
        "    for img_p in split_files:\n",
        "        im = Image.open(img_p)\n",
        "        W, H = im.size\n",
        "        # 复制图片\n",
        "        shutil.copy2(img_p, img_out / img_p.name)\n",
        "        # 写 YOLO 标签\n",
        "        yololines = []\n",
        "        for (cid, x, y, w, h) in parse_per_image_txt(img_p.with_suffix(\".txt\")):\n",
        "            yololines.append(to_yolo_line(cid, (x,y,w,h), W, H))\n",
        "        with open(lbl_out / img_p.with_suffix(\".txt\").name, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\\n\".join(yololines))\n",
        "\n",
        "# 测试集图片复制\n",
        "for img_p in test_imgs:\n",
        "    shutil.copy2(img_p, IMAGES_TEST / img_p.name)\n",
        "\n",
        "# dataset.yaml\n",
        "yaml_text = f\"\"\"\\\n",
        "path: {YOLO_ROOT.as_posix()}\n",
        "train: images/train\n",
        "val: images/val\n",
        "test: images/test\n",
        "\n",
        "nc: {len(NAMES)}\n",
        "names: {NAMES}\n",
        "\"\"\"\n",
        "with open(DATA_YAML, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(yaml_text)\n",
        "\n",
        "print(\"DONE. Wrote\", DATA_YAML)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pLZQkF2RNdtT",
      "metadata": {
        "id": "pLZQkF2RNdtT"
      },
      "outputs": [],
      "source": [
        "#   1) 先运行原本的“转换数据格式”cell（产生 data_yolo/images & labels + dataset.yaml）\n",
        "#   2) 再运行本 cell，会生成：\n",
        "#        - data_yolo/train_upsampled.txt  ← 训练图像列表（含重复行）\n",
        "#        - data_yolo/dataset_upsampled.yaml ← 仅将 train 指向上面的 txt，其他不变\n",
        "#   3) 训练时把 data 参数改成 data_yolo/dataset_upsampled.yaml\n",
        "\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "import random\n",
        "import json\n",
        "\n",
        "YOLO_ROOT = Path(\"data_yolo\")\n",
        "IMAGES_TRAIN = YOLO_ROOT / \"images\" / \"train\"\n",
        "LABELS_TRAIN = YOLO_ROOT / \"labels\" / \"train\"\n",
        "DATA_YAML    = YOLO_ROOT / \"dataset.yaml\"\n",
        "UP_TXT       = YOLO_ROOT / \"train_upsampled.txt\"\n",
        "UP_YAML      = YOLO_ROOT / \"dataset_upsampled.yaml\"\n",
        "\n",
        "# ===== 可调参数（按需改） =====\n",
        "MAX_REP_FACTOR = 3        # 单张图片最大重复次数（含原始那1次）；防止过度上采样\n",
        "POWER_ALPHA    = 1.0      # 稀有类放大力度；1.0 等价于按“max_count / cls_count”；0.5 更温和\n",
        "SEED       = 42           # 随机种子（用于重复的顺序等）\n",
        "PRINT_TOPN     = 10       # 打印前 N 张重复最多的图片，便于检查\n",
        "\n",
        "random.seed(SEED)\n",
        "\n",
        "# 读 dataset.yaml，拿到类名（与原作业一致）\n",
        "import yaml\n",
        "with open(DATA_YAML, \"r\", encoding=\"utf-8\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "names = cfg.get(\"names\", [])\n",
        "nc = int(cfg.get(\"nc\", len(names) or 4))\n",
        "if not names or len(names) != nc:\n",
        "    # 兜底：作业类名固定 4 类\n",
        "    names = [\"car\", \"hov\", \"person\", \"motorcycle\"]\n",
        "cls_names = {i: n for i, n in enumerate(names)}\n",
        "\n",
        "# 扫描训练标签，统计每类框数量 & 每张图包含的类别\n",
        "img_exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".webp\"}\n",
        "label_files = sorted([p for p in LABELS_TRAIN.iterdir() if p.suffix == \".txt\"], key=lambda p: p.stem)\n",
        "\n",
        "per_class_count = Counter()\n",
        "per_image_classes = {}  # img_name -> set(cls_ids)\n",
        "per_image_clsbox  = {}  # img_name -> Counter({cls_id: num_boxes_in_that_image})\n",
        "\n",
        "for lbl in label_files:\n",
        "    img_name = lbl.with_suffix(\"\").name\n",
        "    per_image_classes[img_name] = set()\n",
        "    per_image_clsbox[img_name]  = Counter()\n",
        "    try:\n",
        "        lines = lbl.read_text(encoding=\"utf-8\").strip().splitlines()\n",
        "    except Exception:\n",
        "        lines = []\n",
        "    for ln in lines:\n",
        "        ln = ln.strip()\n",
        "        if not ln:\n",
        "            continue\n",
        "        parts = ln.split()\n",
        "        try:\n",
        "            cid = int(float(parts[0]))\n",
        "        except Exception:\n",
        "            continue\n",
        "        if cid < 0 or cid >= nc:\n",
        "            continue\n",
        "        # YOLO label: cid cx cy w h\n",
        "        per_class_count[cid] += 1\n",
        "        per_image_classes[img_name].add(cid)\n",
        "        per_image_clsbox[img_name][cid] += 1\n",
        "\n",
        "# 若某些图片没有标签（空文件），也要记录，避免丢图\n",
        "for img_p in sorted(IMAGES_TRAIN.iterdir(), key=lambda p: p.name):\n",
        "    if img_p.suffix.lower() not in img_exts:\n",
        "        continue\n",
        "    img_name = img_p.with_suffix(\"\").name\n",
        "    per_image_classes.setdefault(img_name, set())\n",
        "    per_image_clsbox.setdefault(img_name, Counter())\n",
        "\n",
        "# 打印原始分布\n",
        "print(\"== 原始每类框数量 ==\")\n",
        "for cid in range(nc):\n",
        "    print(f\"  {cid}: {cls_names.get(cid, str(cid)):<12}  {per_class_count.get(cid,0)}\")\n",
        "max_count = max(per_class_count.values()) if per_class_count else 1\n",
        "\n",
        "# 计算每类需要的“目标倍率”\n",
        "# 经典思路：希望每类的“可见次数”≈ max_count\n",
        "# ratio_c = (max_count / cls_count_c) ** POWER_ALPHA\n",
        "# 注意：对 0 计数的类保持 ratio=MAX_REP_FACTOR（只在极端冷门时避免无限放大）\n",
        "desire_ratio = {}\n",
        "for cid in range(nc):\n",
        "    cnt = per_class_count.get(cid, 0)\n",
        "    if cnt <= 0:\n",
        "        ratio = MAX_REP_FACTOR  # 没见过的类，顶格上采样\n",
        "    else:\n",
        "        ratio = (max_count / cnt) ** POWER_ALPHA\n",
        "    desire_ratio[cid] = max(1.0, min(float(ratio), float(MAX_REP_FACTOR)))\n",
        "\n",
        "# 为每张图计算“重复次数”\n",
        "# 直观做法：该图包含的所有类别的“需要倍率”的最大值\n",
        "# （这样一张同时含稀有类和常见类的图片，会被按稀有类的倍率来重复）\n",
        "rep_factor_img = {}\n",
        "for img_name, cls_set in per_image_classes.items():\n",
        "    if not cls_set:\n",
        "        # 如果这张图没有任何标注（空框），保持 1 次\n",
        "        rep_factor_img[img_name] = 1\n",
        "        continue\n",
        "    r = 1.0\n",
        "    for c in cls_set:\n",
        "        r = max(r, desire_ratio.get(c, 1.0))\n",
        "    r_int = int(math.ceil(r))\n",
        "    r_int = max(1, min(r_int, MAX_REP_FACTOR))\n",
        "    rep_factor_img[img_name] = r_int\n",
        "\n",
        "# 生成上采样训练列表\n",
        "lines = []\n",
        "for img_p in sorted(IMAGES_TRAIN.iterdir(), key=lambda p: p.name):\n",
        "    if img_p.suffix.lower() not in img_exts:\n",
        "        continue\n",
        "    img_name = img_p.with_suffix(\"\").name\n",
        "    k = rep_factor_img.get(img_name, 1)\n",
        "    # 重复写入 k 行（每行一个图像路径）\n",
        "    for _ in range(k):\n",
        "        lines.append(str(img_p.resolve()))\n",
        "\n",
        "# 写出 txt\n",
        "UP_TXT.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
        "\n",
        "# 写出新的 dataset_upsampled.yaml：把 train 指向上面的 txt，其它与原 dataset.yaml 相同\n",
        "with open(DATA_YAML, \"r\", encoding=\"utf-8\") as f:\n",
        "    base_yaml = yaml.safe_load(f)\n",
        "base_yaml = base_yaml or {}\n",
        "base_yaml[\"train\"] = str(UP_TXT.resolve())\n",
        "with open(UP_YAML, \"w\", encoding=\"utf-8\") as f:\n",
        "    yaml.safe_dump(base_yaml, f, sort_keys=False, allow_unicode=True)\n",
        "\n",
        "# 打印汇总信息\n",
        "dup_counter = Counter(rep_factor_img.values())\n",
        "print(\"\\n== 上采样完成 ==\")\n",
        "print(f\"写出: {UP_TXT}  (共 {len(lines)} 行; 去重前图像数={len(rep_factor_img)})\")\n",
        "print(f\"写出: {UP_YAML}\")\n",
        "print(\"重复次数分布（图片张数）：\", dict(sorted(dup_counter.items())))\n",
        "print(\"\\n前几张重复最多的图片（便于人工检查）：\")\n",
        "top_imgs = sorted(rep_factor_img.items(), key=lambda kv: (-kv[1], kv[0]))[:PRINT_TOPN]\n",
        "for name, k in top_imgs:\n",
        "    print(f\"  {name}: x{k}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-I1cOD6ICE1Q",
      "metadata": {
        "id": "-I1cOD6ICE1Q"
      },
      "source": [
        "訓練"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yfHrT5xsN12K",
      "metadata": {
        "id": "yfHrT5xsN12K"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5358252",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5358252",
        "outputId": "5b19fb6d-e62f-44b5-859f-426d90c1f7fb"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model: 从 YAML 初始化（随机权重）\n",
        "model = YOLO(\"yolo11l.yaml\")\n",
        "\n",
        "# Train the model with custom parameters\n",
        "results = model.train(\n",
        "    data=\"data_yolo/dataset_upsampled.yaml\",\n",
        "    epochs=200,          # 你可根据时间调\n",
        "    imgsz=960,           # 显存吃紧可降到 640/768\n",
        "    batch=1,             # 12GB 建议 8~16\n",
        "    device=0,            # 或\"cpu\"\n",
        "    pretrained=False,    # 禁用预训练\n",
        "    optimizer=\"SGD\",\n",
        "    lr0=0.01,\n",
        "    lrf=0.1,\n",
        "    momentum=0.937,\n",
        "    weight_decay=5e-4,\n",
        "    warmup_epochs=3.0,\n",
        "    cos_lr=True,\n",
        "    project=\"runs/train\",\n",
        "    name=\"hw2\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63719648",
      "metadata": {
        "id": "63719648"
      },
      "source": [
        "驗證集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q6KUQkLbv06N",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6KUQkLbv06N",
        "outputId": "e6e8aea7-b207-4640-fb42-2960c5ace696"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "\n",
        "ckpt = Path(\"/content/runs/train/hw210/weights/best.pt\") # Point to the best.pt file\n",
        "assert ckpt.exists(), f\"未找到模型权重：{ckpt}\"\n",
        "\n",
        "model = YOLO(str(ckpt))  # load a custom model（刚训练出来的）\n",
        "metrics = model.val(data=\"data_yolo/dataset_upsampled.yaml\", imgsz=1920)\n",
        "metrics  # mAP 等指标"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f50d262",
      "metadata": {
        "id": "6f50d262"
      },
      "source": [
        "生成submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "532d0912",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "532d0912",
        "outputId": "d7fdbdea-4040-44cf-bcc2-4152c7e973e0"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# ======= 可调参数 =======\n",
        "MAIN_CONF = 0.01      # 想要的主阈值\n",
        "PRED_CONF = 0.001     # 推理时使用的低阈值，用于抓全候选，后续再按 MAIN_CONF 过滤\n",
        "IOU_THRES = 0.7\n",
        "IMG_SIZE  = 960\n",
        "DEVICE    = 0         # 或\"cpu\"\n",
        "\n",
        "# 载入刚训练的权重\n",
        "ckpt = Path(\"/content/runs/train/hw210/weights/best.pt\")\n",
        "assert ckpt.exists(), f\"未找到模型权重：{ckpt}\"\n",
        "model = YOLO(str(ckpt))\n",
        "\n",
        "# 收集 test 文件并确定 Image_ID（按文件名排序→1-based）\n",
        "IMAGES_TEST = Path(\"data_yolo/images/test\")\n",
        "img_exts = {'.jpg','.jpeg','.png','.bmp','.tif','.tiff','.webp'}\n",
        "test_files = sorted([p for p in IMAGES_TEST.iterdir() if p.suffix.lower() in img_exts], key=lambda p: p.name)\n",
        "assert len(test_files) > 0, f\"测试目录为空：{IMAGES_TEST}\"\n",
        "id_map = {p.name: i+1 for i, p in enumerate(test_files)}  # 1-based\n",
        "\n",
        "# 用较低 conf 推理，确保能拿到“top1 兜底候选”\n",
        "pred_list = list(model.predict(\n",
        "    source=str(IMAGES_TEST),\n",
        "    imgsz=IMG_SIZE,\n",
        "    conf=PRED_CONF,    # 低阈值，保留尽可能多候选\n",
        "    iou=IOU_THRES,\n",
        "    device=DEVICE,\n",
        "    stream=False,\n",
        "    save=True, \n",
        "    verbose=False\n",
        "))\n",
        "\n",
        "# 建索引：文件名 -> 预测结果\n",
        "name_to_pred = {Path(r.path).name: r for r in pred_list}\n",
        "\n",
        "rows = []\n",
        "for p in test_files:\n",
        "    img_name = p.name\n",
        "    image_id = id_map[img_name]\n",
        "\n",
        "    # 读取原图尺寸，保证像素坐标正确\n",
        "    try:\n",
        "        W, H = Image.open(p).size\n",
        "    except Exception:\n",
        "        # 打不开图时，写空串以避免 NaN/null\n",
        "        rows.append({\"Image_ID\": int(image_id), \"PredictionString\": \"\"})\n",
        "        continue\n",
        "\n",
        "    r = name_to_pred.get(img_name, None)\n",
        "\n",
        "    # 若完全没返回结果对象，直接空串\n",
        "    if r is None or getattr(r, \"boxes\", None) is None or len(r.boxes) == 0:\n",
        "        rows.append({\"Image_ID\": int(image_id), \"PredictionString\": \"\"})\n",
        "        continue\n",
        "\n",
        "    xyxy_all = r.boxes.xyxy.detach().cpu().numpy()\n",
        "    conf_all = r.boxes.conf.detach().cpu().numpy()\n",
        "    cls_all  = r.boxes.cls.detach().cpu().numpy().astype(int)\n",
        "\n",
        "    # 先按 MAIN_CONF 过滤\n",
        "    keep = conf_all >= MAIN_CONF\n",
        "    xyxy = xyxy_all[keep]\n",
        "    conf = conf_all[keep]\n",
        "    cls  = cls_all[keep]\n",
        "\n",
        "    # 如果没有任何框通过 MAIN_CONF，则兜底：取该图中置信度最高的那个框（即使低于 MAIN_CONF）\n",
        "    if xyxy.shape[0] == 0 and len(conf_all) > 0:\n",
        "        top_idx = int(np.argmax(conf_all))\n",
        "        xyxy = xyxy_all[top_idx:top_idx+1]\n",
        "        conf = conf_all[top_idx:top_idx+1]\n",
        "        cls  = cls_all[top_idx:top_idx+1]\n",
        "\n",
        "    parts = []\n",
        "    for i in range(len(xyxy)):\n",
        "        x1, y1, x2, y2 = map(float, xyxy[i])\n",
        "        s = float(conf[i])\n",
        "        c = int(cls[i])\n",
        "\n",
        "        # 数值健壮性检查\n",
        "        if not (np.isfinite(x1) and np.isfinite(y1) and np.isfinite(x2) and np.isfinite(y2) and np.isfinite(s)):\n",
        "            continue\n",
        "\n",
        "        w = max(0.0, x2 - x1)\n",
        "        h = max(0.0, y2 - y1)\n",
        "\n",
        "        # 输出格式：<conf> <bb_left> <bb_top> <bb_width> <bb_height> <class>\n",
        "        parts.extend([\n",
        "            f\"{s:.6f}\",\n",
        "            f\"{x1:.2f}\", f\"{y1:.2f}\",\n",
        "            f\"{w:.2f}\",  f\"{h:.2f}\",\n",
        "            str(c)\n",
        "        ])\n",
        "\n",
        "    pred_str = \" \".join(parts) if parts else \"\"   # parts 可能因数值不合法被全过滤\n",
        "    rows.append({\"Image_ID\": int(image_id), \"PredictionString\": pred_str})\n",
        "\n",
        "# 生成 DataFrame，强制无 NaN/null\n",
        "sub = pd.DataFrame(rows, columns=[\"Image_ID\", \"PredictionString\"]).sort_values(\"Image_ID\").reset_index(drop=True)\n",
        "sub[\"Image_ID\"] = sub[\"Image_ID\"].astype(int)\n",
        "sub[\"PredictionString\"] = sub[\"PredictionString\"].astype(str).fillna(\"\")\n",
        "\n",
        "# 保险：若意外缺失某些 ID，补空行\n",
        "expected = set(range(1, len(test_files)+1))\n",
        "actual = set(sub[\"Image_ID\"].tolist())\n",
        "missing = expected - actual\n",
        "if missing:\n",
        "    sub = pd.concat([\n",
        "        sub,\n",
        "        pd.DataFrame({\"Image_ID\": sorted(missing), \"PredictionString\": [\"\"] * len(missing)})\n",
        "    ], ignore_index=True).sort_values(\"Image_ID\").reset_index(drop=True)\n",
        "\n",
        "out_path = \"submission29.csv\"\n",
        "sub.to_csv(out_path, index=False)\n",
        "print(f\"写出 {out_path} 完成！行数={len(sub)}，是否存在空值：\", sub.isna().any().to_dict())\n",
        "sub.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e98c71a4",
      "metadata": {
        "id": "e98c71a4"
      },
      "source": [
        "結果可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f1f1e4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8f1f1e4b",
        "outputId": "83076f80-dcd9-4da1-8300-8f3ccab171ed"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# Example: Ultralytics default save to runs/detect/predict* or runs/predict*/images\n",
        "# Adjust this path based on your Cell #6 save results\n",
        "# Corrected visualization directory path\n",
        "vis_dir = Path(\"/content/runs\") / \"detect\"\n",
        "img_dir = vis_dir / \"predict6\" # Point to the test prediction directory\n",
        "assert img_dir.exists(), f\"Directory does not exist: {img_dir}\"\n",
        "\n",
        "exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
        "files = sorted([p for p in img_dir.iterdir() if p.suffix.lower() in exts], key=lambda p: p.stem)[:5]\n",
        "assert files, f\"No images found in {img_dir}\"\n",
        "\n",
        "print(\"Using directory:\", img_dir)\n",
        "for p in files:\n",
        "    print(p.name)\n",
        "    display(Image.open(p))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
